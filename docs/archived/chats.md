/*
ğŸ§  PROYECTO: Chatbot de Reglamento de TrÃ¡nsito de Hermosillo
ğŸ¯ OBJETIVO: Crear una plataforma web donde los ciudadanos puedan consultar el reglamento vial en espaÃ±ol a travÃ©s de preguntas abiertas y obtener respuestas claras, usando un modelo LLM alojado en Hugging Face.

ğŸ“š FLUJO GENERAL:
1. Usuario escribe pregunta en el frontend (React).
2. Frontend envÃ­a la pregunta al backend FastAPI.
3. FastAPI busca en un archivo JSON (`reglamento.json`) entradas relevantes al tema.
4. Con esas entradas, construye un prompt.
5. Consulta un modelo en espaÃ±ol (`AIDC-AI/Marco-LLM-ES`) desde Hugging Face.
6. Devuelve la respuesta generada al frontend.

ğŸ“¦ STACK COMPLETO:

Frontend:
- React 18 + TypeScript
- Vite 5 + Tailwind CSS
- shadcn/ui (Textarea, Button, ScrollArea)
- `ChatbotReglamento.tsx` como componente principal

Backend:
- FastAPI (Python 3.10+)
- Transformers de Hugging Face
- Modelo: AIDC-AI/Marco-LLM-ES (7B)
- Archivo local `reglamento.json` como base de conocimiento

âš™ï¸ CONFIGURACIÃ“N DEL FRONTEND:

1. Guardar el archivo `reglamento.json` en `src/data/reglamento.json`
2. Crear el componente `ChatbotReglamento.tsx` en `src/components/`
3. El componente debe:
   - Tener Textarea para ingresar pregunta
   - BotÃ³n para enviar consulta
   - ScrollArea para mostrar respuesta
   - LÃ³gica para hacer `POST` a `http://localhost:8000/query` con body JSON: `{ pregunta: "...text..." }`

âœ… Ejemplo de integraciÃ³n:
- Usuario pregunta: â€œÂ¿CuÃ¡l es el lÃ­mite de velocidad en calles secundarias?â€
- Se encuentra entrada relevante en JSON:
  - subcategorÃ­a: Calles secundarias o terciarias
  - descripciÃ³n: 30 km/h en calles secundarias o terciarias.
- Se construye prompt:
  - â€œContexto: Calles secundarias o terciarias: 30 km/h en calles secundarias o terciarias.â€
  - â€œPregunta: Â¿CuÃ¡l es el lÃ­mite de velocidad en calles secundarias?â€
  - Se genera respuesta con LLM y se devuelve al usuario.

ğŸ“ BACKEND FASTAPI (main.py):
- Define un endpoint POST `/query`
- Carga el JSON completo al iniciar
- Filtra entradas del JSON que coincidan con la pregunta
- Construye el prompt y lo manda al modelo desde Hugging Face (con tokenizer y modelo de transformers)
- Devuelve texto limpio como respuesta

ğŸŒ MODELO:
- AIDC-AI/Marco-LLM-ES (espaÃ±ol)
- Licencia Apache-2.0
- Cargado con `transformers.AutoTokenizer` y `AutoModelForCausalLM`
- Se recomienda usar con `torch_dtype=torch.float16` y `device_map='auto'` si hay GPU

ğŸ§ª Prueba local:
- Ejecutar backend: `uvicorn main:app --reload`
- Ejecutar frontend: `npm run dev`
- Ir a `http://localhost:5173`
- Escribir una pregunta
- Ver respuesta generada por el LLM

ğŸ§± ESTRUCTURA DE CARPETAS:

ğŸ“ /frontend
  â””â”€ src/
     â”œâ”€ components/
     â”‚   â””â”€ ChatbotReglamento.tsx
     â”œâ”€ data/
     â”‚   â””â”€ reglamento.json
     â””â”€ App.tsx

ğŸ“ /backend
  â”œâ”€ main.py
  â”œâ”€ reglamento.json
  â””â”€ requirements.txt

ğŸ›  REQUIREMENTS DEL BACKEND:

fastapi
uvicorn
transformers
torch

ğŸš€ OPCIONAL: Dockerfile y despliegue en Render/Vercel

Este sistema debe ser modular, Ã¡gil, liviano y entendible para que un equipo municipal o cÃ­vico pueda replicarlo fÃ¡cilmente en otras ciudades.
*/